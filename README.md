# TinyLlama RAG (Retrieval-Augmented Generation) System

This repository contains two main components:

1. A **Streamlit-based RAG assistant** that uses TinyLlama for natural language queries with document retrieval.
2. An **evaluation script** that measures the model's performance using BLEU, ROUGE, and BERTScore against a dataset of expected answers.

---

## ğŸ“ Directory Structure

```
â”œâ”€â”€ chromadb_index/               # Persistent vector store for document embeddings
â”œâ”€â”€ Docs/                         # Folder with .pdf or .txt documents
â”œâ”€â”€ rag_evaluation_dataset.csv   # CSV with questions and reference answers for evaluation
â”œâ”€â”€ app.py                        # Streamlit-based QA interface
â”œâ”€â”€ evaluat.py                    # Evaluation script with metrics
```

---



## ğŸš€ Streamlit QA App (`code.py`)

This script launches a web interface using Streamlit, allowing users to:

* Input a query.
* Retrieve top-3 relevant document chunks using Chroma vector store.
* Generate a short answer using TinyLlama.
* Show latency metrics for retrieval and LLM generation.

### ğŸ–¥ï¸ Run the app

```bash
streamlit run code.py
```

### ğŸ” Features

* Uses `sentence-transformers/all-MiniLM-L6-v2` for embedding.
* Retrieves top-3 documents and removes duplicate content.
* Uses `TinyLlama/TinyLlama-1.1B-Chat-v1.0` as the language model.
* Displays:

  * Response
  * Snippets from retrieved docs
  * Latency metrics (retrieval, LLM, total time)

---

## ğŸ“Š Evaluation Script (`evaluate.py`)

This script evaluates the end-to-end RAG system using a CSV dataset with queries and reference answers.

### ğŸ“ CSV Format

The CSV should contain:

```
query,expected_answer
"What is RAG?","Retrieval-Augmented Generation is..."
```

### ğŸ“ˆ Metrics Used

* **BLEU** â€“ Evaluates word overlap.
* **ROUGE-1 & ROUGE-L** â€“ Recall-based scoring of n-grams.
* **BERTScore (F1)** â€“ Semantic similarity using pre-trained embeddings.

### â–¶ï¸ Run Evaluation

```bash
python evaluate.py
```

### ğŸ“¤ Output

Example output:

```
 BLEU Score:        0.0108
 ROUGE-1 Score:     0.0706
 ROUGE-L Score:     0.0617
 BERTScore (F1):    0.8097
```

